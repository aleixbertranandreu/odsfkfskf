# Visual Search con Fashion-CLIP üëî‚ú®
**Sistema de Recuperaci√≥n de Im√°genes de Moda usando IA Contrastiva**

<p align="center">
  <img src="https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white" alt="PyTorch">
  <img src="https://img.shields.io/badge/Hugging%20Face-FFD21E?style=for-the-badge&logo=huggingface&logoColor=000" alt="Hugging Face">
  <img src="https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54" alt="Python">
  <img src="https://img.shields.io/badge/YOLO-01B1EC?style=for-the-badge&logo=yolo&logoColor=white" alt="YOLOv8">
</p>

## Overview del Proyecto

Este repositorio contiene el c√≥digo fuente para una soluci√≥n avanzada de **Visual Search** dise√±ada durante la HackUDC 2026. El objetivo principal de este proyecto es resolver el desaf√≠o de encontrar prendas de ropa similares dentro de un inmenso cat√°logo de im√°genes, un problema fundamental en el sector e-commerce (Retail/Fashion).

Para lograr esto, hemos implementado una estrategia de **Fine-Tuning sobre Fashion-CLIP** utilizando _Hard Negative Mining_ y _Contrastive Learning_ (INFO-NCE Loss). Nuestro sistema aprende a mapear im√°genes a un espacio vectorial denso, donde las prendas visual y sem√°nticamente similares se encuentran cercanas, permitiendo realizar b√∫squedas ultra-r√°pidas mediante similitud de coseno con FAISS.

## Arquitectura y Tech Stack

El pipeline ha sido dise√±ado para alto rendimiento y escalabilidad:

- **Core Metric Learning**: Modelo base [Fashion-CLIP](https://huggingface.co/patrickjohncyh/fashion-clip), optimizado con una cabeza de proyecciones discriminativa.
- **Detecci√≥n de Objetos**: YOLOv8 para extraer crops precisos de las prendas y aislar el ruido del fondo (fondos complejos, posados de modelos).
- **Frameworks Principales**: PyTorch, Hugging Face `transformers`, y `timm` (ConvNeXt-Tiny como backbone alternativo/ensemble).
- **B√∫squeda Vectorial**: FAISS (Meta) para indexaci√≥n y re-ranking ultrarr√°pido a gran escala.
- **Hardware de Entrenamiento**: Entrenado en la nube usando instancias **RunPod con GPUs NVIDIA A40** (48GB VRAM), aprovechando Automatic Mixed Precision (AMP / FP16).

## Estructura del Repositorio

```text
HackUDC_2026/
‚îú‚îÄ‚îÄ data/                    # Datasets: cat√°logos de im√°genes, anotaciones y bundles
‚îú‚îÄ‚îÄ docs/                    # Documentaci√≥n t√©cnica y diagramas de arquitectura
‚îú‚îÄ‚îÄ notebooks/               # Jupyter Notebooks para EDA y experimentaci√≥n r√°pida
‚îú‚îÄ‚îÄ src/                     # C√≥digo fuente principal (Production-Ready)
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py           # PyTorch Dataset, DataLoaders y Augmentations
‚îÇ   ‚îú‚îÄ‚îÄ model.py             # Definici√≥n de arquitecturas (Backbones y Projection Heads)
‚îÇ   ‚îú‚îÄ‚îÄ finetune_clip.py     # Script principal de entrenamiento contrastivo
‚îÇ   ‚îú‚îÄ‚îÄ build_index.py       # Generaci√≥n de √≠ndices tensoriales/FAISS
‚îÇ   ‚îú‚îÄ‚îÄ reranker.py          # L√≥gica de re-ranking por reglas duras (Hard Category Mask)
‚îÇ   ‚îú‚îÄ‚îÄ final_inference.py   # Inferencia final y generaci√≥n de submission
‚îÇ   ‚îú‚îÄ‚îÄ utils.py             # Funciones auxiliares gen√©ricas
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ pyproject.toml           # Configuraci√≥n del proyecto y dependencias
‚îú‚îÄ‚îÄ requirements.txt         # Lista de dependencias directas
‚îî‚îÄ‚îÄ README.md                # Este archivo
```

## Instrucciones de Uso

### Instalaci√≥n

Recomendamos utilizar un entorno virtual. El proyecto soporta tanto instalaci√≥n cl√°sica mediante `pip` como la herramienta ultrarr√°pida `uv`.

```bash
# Opci√≥n 1: Usando pip cl√°sico
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Opci√≥n 2: Usando uv (Recomendado)
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
```

### Pipelines de Ejecuci√≥n

**1. Entrenamiento del Modelo (Fine-Tuning)**
Para iniciar el fine-tuning de Fashion-CLIP con tus datos:
```bash
python src/finetune_clip.py --batch_size 64 --epochs 10 --lr 3e-5
```

**2. Construcci√≥n del √çndice Base de Datos Vectorial**
Una vez entrenado, extraemos los embeddings del cat√°logo y construimos el √≠ndice FAISS:
```bash
python src/build_clip_index.py --model_path checkpoints/best_model.pt
```

**3. Inferencia de Evaluaci√≥n (Test Set)**
Genera el archivo CSV final con las recomendaciones priorizadas:
```bash
python src/final_inference.py --index_path faiss_index.index --output submission_final.csv
```

## üöÄ Additional Features: Full-Stack Visual Search Web App

Aunque el n√∫cleo del reto era puramente algor√≠tmico, como reto personal y para darle una vida real al modelo, **desarrollamos un Front-End y Back-End funcional**. Hemos desplegado un entorno web estilo e-commerce (muy al estilo del ecosistema Inditex) para que la experiencia de Visual Search pueda ser testeada en la vida real mediante una Mobile Web App.

- **Frontend (Mobile-First)**: Una interfaz premium, *clean* y minimalista que permite a los usuarios abrir la c√°mara de su m√≥vil, escanear una prenda por la calle y recibir recomendaciones al instante.
- **Backend**: Una API REST r√°pida que conecta nuestro cliente web con el √≠ndice vectorial FAISS y nuestro modelo CLIP entrenado, procesando las inferencias visuales en tiempo real.

### üß† System Architecture Flow

A continuaci√≥n se detalla el ciclo de vida de una petici√≥n de b√∫squeda visual dentro de la aplicaci√≥n, ilustrando c√≥mo intervienen los distintos modelos de IA:

```mermaid
sequenceDiagram
    participant User as üì± Usuario (WebApp)
    participant API as üåê REST API
    participant YOLO as üëÅÔ∏è YOLOv8 (Detector)
    participant CLIP as üß† Fashion-CLIP
    participant FAISS as üóÑÔ∏è FAISS Index

    User->>API: 1. Sube foto de una prenda
    API->>YOLO: 2. Env√≠a imagen para detecci√≥n de ropa
    YOLO-->>API: 3. Devuelve "crop" delimitando la prenda (sin fondo)
    API->>CLIP: 4. Manda el crop al modelo visual
    CLIP-->>API: 5. Genera Embedding Normalizado (512d)
    API->>FAISS: 6. Fast K-NN Search (Cosine Sim.)
    FAISS-->>API: 7. Retorna los Top-K IDs m√°s similares del cat√°logo
    API-->>User: 8. Renderiza productos recomendados
```

## Trabajo Futuro

Debido a las estrictas limitaciones de tiempo inherentes a un formato de hackathon de 24/48 horas, varias ideas prometedoras quedaron en el tintero. Nuestro roadmap de mejoras incluye:

1. **Self-Supervised Pre-training Extendido**: Aprovechar la inmensa cantidad de im√°genes sin etiquetar del cat√°logo para pre-entrenar con DINOv2 / MAE antes del fine-tuning contrastivo.
2. **Atenci√≥n Multimodal**: Incorporar metadata de texto (descripci√≥n del producto, color, material) fusionando las representaciones de texto y visi√≥n de CLIP.
3. **Despliegue Backend (API)**: Empaquetar el sistema de inferencia usando FastAPI y contenedorizaci√≥n con Docker, sirviendo las b√∫squedas visuales a trav√©s de endpoints REST en tiempo real (< 100ms).
4. **Knowledge Distillation**: Destilar el modelo final pesado (ensemble) en una red m√°s peque√±a de forma que pueda ejecutarse en el edge (ej., aplicaci√≥n m√≥vil).
