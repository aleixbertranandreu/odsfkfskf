# Visual Search con Fashion-CLIP ğŸ‘”âœ¨
**Sistema de RecuperaciÃ³n de ImÃ¡genes de Moda usando IA Contrastiva**

## Overview del Proyecto

Este repositorio contiene el cÃ³digo fuente para una soluciÃ³n avanzada de **Visual Search** diseÃ±ada durante la HackUDC 2026. El objetivo principal de este proyecto es resolver el desafÃ­o de encontrar prendas de ropa similares dentro de un inmenso catÃ¡logo de imÃ¡genes, un problema fundamental en el sector e-commerce (Retail/Fashion).

Para lograr esto, hemos implementado una estrategia de **Fine-Tuning sobre Fashion-CLIP** utilizando _Hard Negative Mining_ y _Contrastive Learning_ (INFO-NCE Loss). Nuestro sistema aprende a mapear imÃ¡genes a un espacio vectorial denso, donde las prendas visual y semÃ¡nticamente similares se encuentran cercanas, permitiendo realizar bÃºsquedas ultra-rÃ¡pidas mediante similitud de coseno con FAISS.

## Arquitectura y Tech Stack

El pipeline ha sido diseÃ±ado para alto rendimiento y escalabilidad:

- **Core Metric Learning**: Modelo base [Fashion-CLIP](https://huggingface.co/patrickjohncyh/fashion-clip), optimizado con una cabeza de proyecciones discriminativa.
- **DetecciÃ³n de Objetos**: YOLOv8 para extraer crops precisos de las prendas y aislar el ruido del fondo (fondos complejos, posados de modelos).
- **Frameworks Principales**: PyTorch, Hugging Face `transformers`, y `timm` (ConvNeXt-Tiny como backbone alternativo/ensemble).
- **BÃºsqueda Vectorial**: FAISS (Meta) para indexaciÃ³n y re-ranking ultrarrÃ¡pido a gran escala.
- **Hardware de Entrenamiento**: Entrenado en la nube usando instancias **RunPod con GPUs NVIDIA A40** (48GB VRAM), aprovechando Automatic Mixed Precision (AMP / FP16).

## Estructura del Repositorio

```text
HackUDC_2026/
â”œâ”€â”€ data/                    # Datasets: catÃ¡logos de imÃ¡genes, anotaciones y bundles
â”œâ”€â”€ docs/                    # DocumentaciÃ³n tÃ©cnica y diagramas de arquitectura
â”œâ”€â”€ notebooks/               # Jupyter Notebooks para EDA y experimentaciÃ³n rÃ¡pida
â”œâ”€â”€ src/                     # CÃ³digo fuente principal (Production-Ready)
â”‚   â”œâ”€â”€ dataset.py           # PyTorch Dataset, DataLoaders y Augmentations
â”‚   â”œâ”€â”€ model.py             # DefiniciÃ³n de arquitecturas (Backbones y Projection Heads)
â”‚   â”œâ”€â”€ finetune_clip.py     # Script principal de entrenamiento contrastivo
â”‚   â”œâ”€â”€ build_index.py       # GeneraciÃ³n de Ã­ndices tensoriales/FAISS
â”‚   â”œâ”€â”€ reranker.py          # LÃ³gica de re-ranking por reglas duras (Hard Category Mask)
â”‚   â”œâ”€â”€ final_inference.py   # Inferencia final y generaciÃ³n de submission
â”‚   â”œâ”€â”€ utils.py             # Funciones auxiliares genÃ©ricas
â”‚   â””â”€â”€ ...
â”œâ”€â”€ pyproject.toml           # ConfiguraciÃ³n del proyecto y dependencias
â”œâ”€â”€ requirements.txt         # Lista de dependencias directas
â””â”€â”€ README.md                # Este archivo
```

## Instrucciones de Uso

### InstalaciÃ³n

Recomendamos utilizar un entorno virtual. El proyecto soporta tanto instalaciÃ³n clÃ¡sica mediante `pip` como la herramienta ultrarrÃ¡pida `uv`.

```bash
# OpciÃ³n 1: Usando pip clÃ¡sico
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# OpciÃ³n 2: Usando uv (Recomendado)
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
```

### Pipelines de EjecuciÃ³n

**1. Entrenamiento del Modelo (Fine-Tuning)**
Para iniciar el fine-tuning de Fashion-CLIP con tus datos:
```bash
python src/finetune_clip.py --batch_size 64 --epochs 10 --lr 3e-5
```

**2. ConstrucciÃ³n del Ãndice Base de Datos Vectorial**
Una vez entrenado, extraemos los embeddings del catÃ¡logo y construimos el Ã­ndice FAISS:
```bash
python src/build_clip_index.py --model_path checkpoints/best_model.pt
```

**3. Inferencia de EvaluaciÃ³n (Test Set)**
Genera el archivo CSV final con las recomendaciones priorizadas:
```bash
python src/final_inference.py --index_path faiss_index.index --output submission_final.csv
```

## Trabajo Futuro

Debido a las estrictas limitaciones de tiempo inherentes a un formato de hackathon de 24/48 horas, varias ideas prometedoras quedaron en el tintero. Nuestro roadmap de mejoras incluye:

1. **Self-Supervised Pre-training Extendido**: Aprovechar la inmensa cantidad de imÃ¡genes sin etiquetar del catÃ¡logo para pre-entrenar con DINOv2 / MAE antes del fine-tuning contrastivo.
2. **AtenciÃ³n Multimodal**: Incorporar metadata de texto (descripciÃ³n del producto, color, material) fusionando las representaciones de texto y visiÃ³n de CLIP.
3. **Despliegue Backend (API)**: Empaquetar el sistema de inferencia usando FastAPI y contenedorizaciÃ³n con Docker, sirviendo las bÃºsquedas visuales a travÃ©s de endpoints REST en tiempo real (< 100ms).
4. **Knowledge Distillation**: Destilar el modelo final pesado (ensemble) en una red mÃ¡s pequeÃ±a de forma que pueda ejecutarse en el edge (ej., aplicaciÃ³n mÃ³vil).
