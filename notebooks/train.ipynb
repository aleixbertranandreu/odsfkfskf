{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4df960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memoria de la GPU liberada.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# 1. Borramos variables pesadas si existen\n",
    "if 'model' in locals(): del model\n",
    "if 'owl_model' in locals(): del owl_model\n",
    "\n",
    "# 2. Vaciamos la cach√© de CUDA\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"üßπ Memoria de la GPU liberada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1dc5321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Carpeta encontrada. Hay 276 im√°genes listas para entrenar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleixbertranandreu/Documents/HackUDC_2026/.venv/lib64/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aleixbertranandreu/Documents/HackUDC_2026/.venv/lib64/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Empezando entrenamiento con 276 pares de im√°genes...\n",
      "Epoch 1/5 - Loss: 0.0301\n",
      "Epoch 2/5 - Loss: 0.0031\n",
      "Epoch 3/5 - Loss: 0.0017\n",
      "Epoch 4/5 - Loss: 0.0011\n",
      "Epoch 5/5 - Loss: 0.0008\n",
      "‚úÖ ¬°Modelo guardado como /home/aleixbertranandreu/Documents/HackUDC_2026/modelo_experto_inditex.pt!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Usamos la ruta absoluta de tu proyecto\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/HackUDC_2026\")\n",
    "CARPETA_RECORTES = os.path.join(BASE_DIR, \"data\", \"dataset_entrenar_IA\")\n",
    "CARPETA_CATALOGO = os.path.join(BASE_DIR, \"data\", \"images\", \"products\")\n",
    "MODELO_SALIDA = os.path.join(BASE_DIR, \"modelo_experto_inditex.pt\")\n",
    "\n",
    "# Verificaci√≥n de seguridad antes de empezar\n",
    "if not os.path.exists(CARPETA_RECORTES):\n",
    "    print(f\"‚ùå ERROR: No existe la carpeta de recortes en {CARPETA_RECORTES}\")\n",
    "else:\n",
    "    n_fotos = len([f for f in os.listdir(CARPETA_RECORTES) if f.endswith('.jpg')])\n",
    "    print(f\"‚úÖ Carpeta encontrada. Hay {n_fotos} im√°genes listas para entrenar.\")\n",
    "\n",
    "# --- 2. EL DATASET \"ESPEJO\" ---\n",
    "class InditexContrastiveDataset(Dataset):\n",
    "    def __init__(self, carpeta_recortes, carpeta_catalogo, transform=None):\n",
    "        self.carpeta_recortes = carpeta_recortes\n",
    "        self.carpeta_catalogo = carpeta_catalogo\n",
    "        self.transform = transform\n",
    "        # Listamos los recortes que acabas de generar\n",
    "        self.lista_recortes = [f for f in os.listdir(carpeta_recortes) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lista_recortes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        nombre_recorte = self.lista_recortes[idx]\n",
    "        # Extraemos el ID de producto del nombre (ej: I_476e19..._from_B_...)\n",
    "        id_producto = nombre_recorte.split('_from_')[0]\n",
    "        \n",
    "        ruta_recorte = os.path.join(self.carpeta_recortes, nombre_recorte)\n",
    "        ruta_catalogo = os.path.join(self.carpeta_catalogo, f\"{id_producto}.jpg\")\n",
    "        \n",
    "        # Cargamos ambas im√°genes\n",
    "        img_real = Image.open(ruta_recorte).convert(\"RGB\")\n",
    "        try:\n",
    "            img_estudio = Image.open(ruta_catalogo).convert(\"RGB\")\n",
    "        except:\n",
    "            # Si falta la del cat√°logo, usamos la misma (solo para evitar errores)\n",
    "            img_estudio = img_real\n",
    "            \n",
    "        if self.transform:\n",
    "            img_real = self.transform(img_real)\n",
    "            img_estudio = self.transform(img_estudio)\n",
    "            \n",
    "        return img_real, img_estudio\n",
    "\n",
    "# --- 3. PREPARACI√ìN ---\n",
    "transformaciones = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = InditexContrastiveDataset(CARPETA_RECORTES, CARPETA_CATALOGO, transform=transformaciones)\n",
    "# Cambiamos batch_size de 32 a 8 o incluso 4 si sigue petando\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "# Usamos una ResNet50 como base (muy buena para texturas de ropa)\n",
    "modelo = models.resnet50(pretrained=True)\n",
    "# Cambiamos la √∫ltima capa para que escupa un vector de 512 dimensiones (como CLIP)\n",
    "modelo.fc = nn.Linear(modelo.fc.in_features, 512)\n",
    "modelo = modelo.to(device)\n",
    "\n",
    "criterion = nn.CosineEmbeddingLoss() # Esta p√©rdida es perfecta para comparar vectores\n",
    "optimizer = optim.Adam(modelo.parameters(), lr=0.0001)\n",
    "\n",
    "# --- 4. BUCLE DE ENTRENAMIENTO (¬°A por el oro!) ---\n",
    "print(f\"üî• Empezando entrenamiento con {len(dataset)} pares de im√°genes...\")\n",
    "modelo.train()\n",
    "\n",
    "for epoch in range(5): # Con 5 √©pocas para empezar sobra\n",
    "    running_loss = 0.0\n",
    "    for img_real, img_estudio in loader:\n",
    "        img_real, img_estudio = img_real.to(device), img_estudio.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Sacamos los vectores de ambas fotos\n",
    "        vec_real = modelo(img_real)\n",
    "        vec_estudio = modelo(img_estudio)\n",
    "        \n",
    "        # Target 1 significa: \"haz que estos dos vectores se parezcan\"\n",
    "        target = torch.ones(vec_real.size(0)).to(device)\n",
    "        \n",
    "        loss = criterion(vec_real, vec_estudio, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/5 - Loss: {running_loss/len(loader):.4f}\")\n",
    "\n",
    "# Guardamos tu nuevo modelo experto\n",
    "torch.save(modelo.state_dict(), MODELO_SALIDA)\n",
    "print(f\"‚úÖ ¬°Modelo guardado como {MODELO_SALIDA}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510f729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ∞Ô∏è Generando base de datos de huellas dactilares del cat√°logo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5604/5604 [03:58<00:00, 23.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Galer√≠a guardada con 5604 productos lista para buscar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. CONFIGURACI√ìN ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/HackUDC_2026\")\n",
    "CARPETA_PRODUCTOS = os.path.join(BASE_DIR, \"data\", \"images\", \"products\")\n",
    "RUTA_MODELO = os.path.join(BASE_DIR, \"modelo_experto_inditex.pt\")\n",
    "\n",
    "# --- 2. CARGAR TU MODELO EXPERTO ---\n",
    "modelo = models.resnet50()\n",
    "modelo.fc = nn.Linear(modelo.fc.in_features, 512)\n",
    "modelo.load_state_dict(torch.load(RUTA_MODELO))\n",
    "modelo = modelo.to(device)\n",
    "modelo.eval() # Modo \"solo lectura\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 3. GENERAR LA GALER√çA ---\n",
    "nombres_productos = []\n",
    "embeddings_lista = []\n",
    "\n",
    "print(\"üõ∞Ô∏è Generando base de datos de huellas dactilares del cat√°logo...\")\n",
    "fotos_catalogo = [f for f in os.listdir(CARPETA_PRODUCTOS) if f.endswith('.jpg')]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for nombre in tqdm(fotos_catalogo):\n",
    "        ruta = os.path.join(CARPETA_PRODUCTOS, nombre)\n",
    "        try:\n",
    "            img = Image.open(ruta).convert(\"RGB\")\n",
    "            img_t = transform(img).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Sacamos el vector de 512 dimensiones\n",
    "            vector = modelo(img_t)\n",
    "            \n",
    "            embeddings_lista.append(vector.cpu())\n",
    "            nombres_productos.append(nombre.replace(\".jpg\", \"\"))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Guardamos todo en un solo bloque para que la b√∫squeda sea instant√°nea\n",
    "galeria_vectores = torch.cat(embeddings_lista)\n",
    "torch.save({'vectores': galeria_vectores, 'ids': nombres_productos}, \"galeria_expert.pt\")\n",
    "\n",
    "print(f\"‚úÖ Galer√≠a guardada con {len(nombres_productos)} productos lista para buscar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135c4f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ∞Ô∏è Generando galer√≠a en: /home/aleixbertranandreu/Documents/HackUDC_2026/galeria_expert.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5604/5604 [04:16<00:00, 21.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ¬°LOGRADO! Galer√≠a guardada con 5604 productos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. CONFIGURACI√ìN ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/HackUDC_2026\")\n",
    "CARPETA_PRODUCTOS = os.path.join(BASE_DIR, \"data/images/products\")\n",
    "RUTA_MODELO = os.path.join(BASE_DIR, \"modelo_experto_inditex.pt\")\n",
    "# üî• RUTA ABSOLUTA PARA QUE NO SE PIERDA üî•\n",
    "RUTA_GALERIA_SALIDA = os.path.join(BASE_DIR, \"galeria_expert.pt\")\n",
    "\n",
    "# --- 2. CARGAR TU MODELO EXPERTO ---\n",
    "modelo = models.resnet50()\n",
    "modelo.fc = nn.Linear(modelo.fc.in_features, 512)\n",
    "modelo.load_state_dict(torch.load(RUTA_MODELO, map_location=device))\n",
    "modelo = modelo.to(device)\n",
    "modelo.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "nombres_productos = []\n",
    "embeddings_lista = []\n",
    "\n",
    "print(f\"üõ∞Ô∏è Generando galer√≠a en: {RUTA_GALERIA_SALIDA}\")\n",
    "fotos_catalogo = [f for f in os.listdir(CARPETA_PRODUCTOS) if f.endswith('.jpg')]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for nombre in tqdm(fotos_catalogo):\n",
    "        ruta = os.path.join(CARPETA_PRODUCTOS, nombre)\n",
    "        try:\n",
    "            img = Image.open(ruta).convert(\"RGB\")\n",
    "            img_t = transform(img).unsqueeze(0).to(device)\n",
    "            vector = modelo(img_t)\n",
    "            embeddings_lista.append(vector.cpu())\n",
    "            nombres_productos.append(nombre.replace(\".jpg\", \"\"))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# --- 3. GUARDAR ---\n",
    "if len(embeddings_lista) > 0:\n",
    "    galeria_vectores = torch.cat(embeddings_lista)\n",
    "    torch.save({'vectores': galeria_vectores, 'ids': nombres_productos}, RUTA_GALERIA_SALIDA)\n",
    "    print(f\"‚úÖ ¬°LOGRADO! Galer√≠a guardada con {len(nombres_productos)} productos.\")\n",
    "else:\n",
    "    print(\"‚ùå ERROR: No se encontraron fotos en la carpeta de productos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Archivos en tu carpeta ra√≠z:\n",
      " -> modelo_experto_inditex.pt\n",
      " -> best.pt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f72a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando generaci√≥n de submission en cuda...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1e339196f546cd89c02bff51455a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mOwlViTForObjectDetection LOAD REPORT\u001b[0m from: google/owlvit-base-patch32\n",
      "Key                                         | Status     |  | \n",
      "--------------------------------------------+------------+--+-\n",
      "owlvit.vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "owlvit.text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìë Procesando 455 bundles para la entrega final...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 455/455 [00:51<00:00,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ ¬°SUBMISSION GENERADA! Archivo listo en: /home/aleixbertranandreu/Documents/HackUDC_2026/submission_chiringuito.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. CONFIGURACI√ìN FINAL ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/HackUDC_2026\")\n",
    "RUTA_MODELO = os.path.join(BASE_DIR, \"modelo_experto_inditex.pt\")\n",
    "RUTA_GALERIA = os.path.join(BASE_DIR, \"galeria_expert.pt\")\n",
    "CARPETA_BUNDLES = os.path.join(BASE_DIR, \"data/images/bundles\")\n",
    "RUTA_TEST_CSV = os.path.join(BASE_DIR, \"data/raw/bundles_product_match_test.csv\")\n",
    "ARCHIVO_SALIDA = os.path.join(BASE_DIR, \"submission_chiringuito.csv\")\n",
    "\n",
    "print(f\"üöÄ Iniciando generaci√≥n de submission en {device}...\")\n",
    "\n",
    "# --- 2. CARGAR MODELOS Y GALER√çA ---\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "owl_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\").to(device)\n",
    "\n",
    "expert_model = models.resnet50()\n",
    "expert_model.fc = nn.Linear(expert_model.fc.in_features, 512)\n",
    "expert_model.load_state_dict(torch.load(RUTA_MODELO, map_location=device))\n",
    "expert_model = expert_model.to(device).eval()\n",
    "\n",
    "data_galeria = torch.load(RUTA_GALERIA, map_location=device)\n",
    "galeria_vectores = F.normalize(data_galeria['vectores'].to(device), p=2, dim=1)\n",
    "galeria_ids = data_galeria['ids']\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 3. PROCESAR EL TEST SET ---\n",
    "df_test = pd.read_csv(RUTA_TEST_CSV)\n",
    "bundles_test = df_test['bundle_asset_id'].unique()\n",
    "\n",
    "resultados = []\n",
    "\n",
    "print(f\"üìë Procesando {len(bundles_test)} bundles para la entrega final...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bundle_id in tqdm(bundles_test):\n",
    "        ruta_img = os.path.join(CARPETA_BUNDLES, f\"{bundle_id}.jpg\")\n",
    "        if not os.path.exists(ruta_img):\n",
    "            # Si no hay foto, ponemos 15 IDs vac√≠os o aleatorios (mejor que nada)\n",
    "            resultados.append([bundle_id] + [\"\"] * 15)\n",
    "            continue\n",
    "            \n",
    "        img = Image.open(ruta_img).convert(\"RGB\")\n",
    "        \n",
    "        # Intentamos detectar CUALQUIER prenda (upper o lower)\n",
    "        etiquetas = [[\"upper clothing\", \"lower clothing\", \"dress\"]]\n",
    "        inputs = processor(text=etiquetas, images=img, return_tensors=\"pt\").to(device)\n",
    "        outputs = owl_model(**inputs)\n",
    "        \n",
    "        target_sizes = torch.tensor([img.size[::-1]])\n",
    "        dets = processor.post_process_grounded_object_detection(\n",
    "            outputs=outputs, target_sizes=target_sizes, text_labels=etiquetas, threshold=0.1\n",
    "        )[0]\n",
    "        \n",
    "        if len(dets[\"boxes\"]) > 0:\n",
    "            # Cogemos la detecci√≥n m√°s segura\n",
    "            box = dets[\"boxes\"][0]\n",
    "            x1, y1, x2, y2 = map(int, box.tolist())\n",
    "            recorte = img.crop((x1, y1, x2, y2))\n",
    "            \n",
    "            # Sacamos embedding\n",
    "            rec_t = transform(recorte).unsqueeze(0).to(device)\n",
    "            query_emb = F.normalize(expert_model(rec_t), p=2, dim=1)\n",
    "            \n",
    "            # Buscamos en galer√≠a\n",
    "            sims = torch.mm(query_emb, galeria_vectores.t())\n",
    "            _, indices = torch.topk(sims, 15)\n",
    "            \n",
    "            top_15 = [galeria_ids[idx] for idx in indices[0]]\n",
    "        else:\n",
    "            # Si no detecta nada, devolvemos los 15 primeros de la galer√≠a (fail-safe)\n",
    "            top_15 = galeria_ids[:15]\n",
    "            \n",
    "        resultados.append([bundle_id] + top_15)\n",
    "\n",
    "# --- 4. GUARDAR RESULTADOS ---\n",
    "columnas = ['bundle_asset_id'] + [f'product_asset_id_{i}' for i in range(1, 16)]\n",
    "df_final = pd.DataFrame(resultados, columns=columnas)\n",
    "df_final.to_csv(ARCHIVO_SALIDA, index=False)\n",
    "\n",
    "print(f\"\\nüèÜ ¬°SUBMISSION GENERADA! Archivo listo en: {ARCHIVO_SALIDA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e08e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Cargando motores...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b266cb1edfc24e3096c09cbfc1152d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mOwlViTForObjectDetection LOAD REPORT\u001b[0m from: google/owlvit-base-patch32\n",
      "Key                                         | Status     |  | \n",
      "--------------------------------------------+------------+--+-\n",
      "owlvit.vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "owlvit.text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ‚Äç‚ôÇÔ∏è Procesando 455 bundles para el formato oficial...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 455/455 [00:46<00:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ ¬°SUBMISSION LISTA! Formato de 6825 filas generado en: /home/aleixbertranandreu/Documents/HackUDC_2026/submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. CONFIGURACI√ìN ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE_DIR = os.path.expanduser(\"~/Documents/HackUDC_2026\")\n",
    "RUTA_MODELO = os.path.join(BASE_DIR, \"modelo_experto_inditex.pt\")\n",
    "RUTA_GALERIA = os.path.join(BASE_DIR, \"galeria_expert.pt\")\n",
    "CARPETA_BUNDLES = os.path.join(BASE_DIR, \"data/images/bundles\")\n",
    "# Usamos el archivo de TEST que es el que hay que predecir\n",
    "RUTA_TEST_CSV = os.path.join(BASE_DIR, \"data/raw/bundles_product_match_test.csv\")\n",
    "ARCHIVO_SALIDA = os.path.join(BASE_DIR, \"submission.csv\")\n",
    "\n",
    "# --- 2. CARGA DE MODELOS ---\n",
    "print(\"‚è≥ Cargando motores...\")\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "owl_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\").to(device)\n",
    "\n",
    "expert_model = models.resnet50()\n",
    "expert_model.fc = nn.Linear(expert_model.fc.in_features, 512)\n",
    "expert_model.load_state_dict(torch.load(RUTA_MODELO, map_location=device))\n",
    "expert_model = expert_model.to(device).eval()\n",
    "\n",
    "data_galeria = torch.load(RUTA_GALERIA, map_location=device)\n",
    "galeria_vectores = F.normalize(data_galeria['vectores'].to(device), p=2, dim=1)\n",
    "galeria_ids = data_galeria['ids']\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 3. PROCESADO ---\n",
    "df_test = pd.read_csv(RUTA_TEST_CSV)\n",
    "bundles_test = df_test['bundle_asset_id'].unique()\n",
    "\n",
    "rows = [] # Aqu√≠ guardaremos parejas [bundle, producto]\n",
    "\n",
    "print(f\"üèÉ‚Äç‚ôÇÔ∏è Procesando {len(bundles_test)} bundles para el formato oficial...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bundle_id in tqdm(bundles_test):\n",
    "        ruta_img = os.path.join(CARPETA_BUNDLES, f\"{bundle_id}.jpg\")\n",
    "        \n",
    "        # Si no existe la foto, metemos 15 vac√≠os para no romper el CSV\n",
    "        if not os.path.exists(ruta_img):\n",
    "            for _ in range(15):\n",
    "                rows.append({'bundle_asset_id': bundle_id, 'product_asset_id': \"\"})\n",
    "            continue\n",
    "            \n",
    "        img = Image.open(ruta_img).convert(\"RGB\")\n",
    "        \n",
    "        # Detecci√≥n r√°pida\n",
    "        etiquetas = [[\"upper clothing\", \"lower clothing\", \"dress\"]]\n",
    "        inputs = processor(text=etiquetas, images=img, return_tensors=\"pt\").to(device)\n",
    "        outputs = owl_model(**inputs)\n",
    "        \n",
    "        target_sizes = torch.tensor([img.size[::-1]])\n",
    "        dets = processor.post_process_grounded_object_detection(\n",
    "            outputs=outputs, target_sizes=target_sizes, text_labels=etiquetas, threshold=0.1\n",
    "        )[0]\n",
    "        \n",
    "        if len(dets[\"boxes\"]) > 0:\n",
    "            box = dets[\"boxes\"][0]\n",
    "            x1, y1, x2, y2 = map(int, box.tolist())\n",
    "            recorte = img.crop((x1, y1, x2, y2))\n",
    "            \n",
    "            rec_t = transform(recorte).unsqueeze(0).to(device)\n",
    "            query_emb = F.normalize(expert_model(rec_t), p=2, dim=1)\n",
    "            \n",
    "            sims = torch.mm(query_emb, galeria_vectores.t())\n",
    "            _, indices = torch.topk(sims, 15)\n",
    "            top_15 = [galeria_ids[idx] for idx in indices[0]]\n",
    "        else:\n",
    "            # Fallback si no detecta nada\n",
    "            top_15 = galeria_ids[:15]\n",
    "            \n",
    "        # üåü LA CLAVE: A√±adimos una fila por cada producto\n",
    "        for prod_id in top_15:\n",
    "            rows.append({'bundle_asset_id': bundle_id, 'product_asset_id': prod_id})\n",
    "\n",
    "# --- 4. GUARDAR ---\n",
    "df_final = pd.DataFrame(rows)\n",
    "df_final.to_csv(ARCHIVO_SALIDA, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ ¬°SUBMISSION LISTA! Formato de {len(df_final)} filas generado en: {ARCHIVO_SALIDA}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
